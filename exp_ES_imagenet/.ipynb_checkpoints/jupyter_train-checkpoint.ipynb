{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3,4,5,6 python -m torch.distributed.launch --nproc_per_node=4 example_Imagenet_res18.py\n",
    "#预训练于imagenet完整版\n",
    "#精度：64%，只训练了一小段时间，一次learningrate decay，避免对imagenet拟合过多 batchsize=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "1  is ready\n",
      "2  is ready\n",
      "3  is ready\n",
      "0  is ready\n",
      "start recording\n",
      "Total number of paramerters in networks is 11696369  \n",
      "Total number of paramerters in networks is 11696369  \n",
      "Total number of paramerters in networks is 11696369  \n",
      "Total number of paramerters in networks is 11696369  \n",
      "===> pre_training models...\n",
      "Epoch [0/60], Step [10/15966], Loss: 61.78063 \n",
      "\n",
      "Time elasped: 11 \n",
      "\n",
      "Epoch [0/60], Step [10/15966], acc: 0.04000 \n",
      "\n",
      "Epoch [0/60], Step [20/15966], Loss: 52.81896 \n",
      "\n",
      "Time elasped: 20 \n",
      "\n",
      "Epoch [0/60], Step [20/15966], acc: 0.11500 \n",
      "\n",
      "Epoch [0/60], Step [30/15966], Loss: 52.15116 \n",
      "\n",
      "Time elasped: 30 \n",
      "\n",
      "Epoch [0/60], Step [30/15966], acc: 0.11500 \n",
      "\n",
      "Epoch [0/60], Step [40/15966], Loss: 51.06059 \n",
      "\n",
      "Time elasped: 40 \n",
      "\n",
      "Epoch [0/60], Step [40/15966], acc: 0.10500 \n",
      "\n",
      "Epoch [0/60], Step [50/15966], Loss: 49.77385 \n",
      "\n",
      "Time elasped: 49 \n",
      "\n",
      "Epoch [0/60], Step [50/15966], acc: 0.10500 \n",
      "\n",
      "Epoch [0/60], Step [60/15966], Loss: 49.36038 \n",
      "\n",
      "Time elasped: 59 \n",
      "\n",
      "Epoch [0/60], Step [60/15966], acc: 0.16000 \n",
      "\n",
      "Epoch [0/60], Step [70/15966], Loss: 47.11653 \n",
      "\n",
      "Time elasped: 69 \n",
      "\n",
      "Epoch [0/60], Step [70/15966], acc: 0.17000 \n",
      "\n",
      "Epoch [0/60], Step [80/15966], Loss: 50.41523 \n",
      "\n",
      "Time elasped: 78 \n",
      "\n",
      "Epoch [0/60], Step [80/15966], acc: 0.10000 \n",
      "\n",
      "Epoch [0/60], Step [90/15966], Loss: 50.70852 \n",
      "\n",
      "Time elasped: 88 \n",
      "\n",
      "Epoch [0/60], Step [90/15966], acc: 0.16500 \n",
      "\n",
      "Epoch [0/60], Step [100/15966], Loss: 47.57322 \n",
      "\n",
      "Time elasped: 98 \n",
      "\n",
      "Epoch [0/60], Step [100/15966], acc: 0.18000 \n",
      "\n",
      "Epoch [0/60], Step [110/15966], Loss: 45.60737 \n",
      "\n",
      "Time elasped: 108 \n",
      "\n",
      "Epoch [0/60], Step [110/15966], acc: 0.19000 \n",
      "\n",
      "Epoch [0/60], Step [120/15966], Loss: 46.91839 \n",
      "\n",
      "Time elasped: 117 \n",
      "\n",
      "Epoch [0/60], Step [120/15966], acc: 0.11000 \n",
      "\n",
      "Epoch [0/60], Step [130/15966], Loss: 44.57903 \n",
      "\n",
      "Time elasped: 127 \n",
      "\n",
      "Epoch [0/60], Step [130/15966], acc: 0.15000 \n",
      "\n",
      "Epoch [0/60], Step [140/15966], Loss: 49.33787 \n",
      "\n",
      "Time elasped: 137 \n",
      "\n",
      "Epoch [0/60], Step [140/15966], acc: 0.12000 \n",
      "\n",
      "Epoch [0/60], Step [150/15966], Loss: 49.97133 \n",
      "\n",
      "Time elasped: 146 \n",
      "\n",
      "Epoch [0/60], Step [150/15966], acc: 0.10000 \n",
      "\n",
      "Epoch [0/60], Step [160/15966], Loss: 46.95478 \n",
      "\n",
      "Time elasped: 156 \n",
      "\n",
      "Epoch [0/60], Step [160/15966], acc: 0.14500 \n",
      "\n",
      "Epoch [0/60], Step [170/15966], Loss: 46.34709 \n",
      "\n",
      "Time elasped: 166 \n",
      "\n",
      "Epoch [0/60], Step [170/15966], acc: 0.16000 \n",
      "\n",
      "Epoch [0/60], Step [180/15966], Loss: 46.83424 \n",
      "\n",
      "Time elasped: 175 \n",
      "\n",
      "Epoch [0/60], Step [180/15966], acc: 0.14500 \n",
      "\n",
      "Epoch [0/60], Step [190/15966], Loss: 47.72164 \n",
      "\n",
      "Time elasped: 185 \n",
      "\n",
      "Epoch [0/60], Step [190/15966], acc: 0.14500 \n",
      "\n",
      "Epoch [0/60], Step [200/15966], Loss: 45.80036 \n",
      "\n",
      "Time elasped: 195 \n",
      "\n",
      "Epoch [0/60], Step [200/15966], acc: 0.18500 \n",
      "\n",
      "Epoch [0/60], Step [210/15966], Loss: 43.97171 \n",
      "\n",
      "Time elasped: 205 \n",
      "\n",
      "Epoch [0/60], Step [210/15966], acc: 0.18500 \n",
      "\n",
      "Epoch [0/60], Step [220/15966], Loss: 46.03839 \n",
      "\n",
      "Time elasped: 214 \n",
      "\n",
      "Epoch [0/60], Step [220/15966], acc: 0.13500 \n",
      "\n",
      "Epoch [0/60], Step [230/15966], Loss: 46.84533 \n",
      "\n",
      "Time elasped: 224 \n",
      "\n",
      "Epoch [0/60], Step [230/15966], acc: 0.14500 \n",
      "\n",
      "Epoch [0/60], Step [240/15966], Loss: 43.02113 \n",
      "\n",
      "Time elasped: 234 \n",
      "\n",
      "Epoch [0/60], Step [240/15966], acc: 0.18500 \n",
      "\n",
      "Epoch [0/60], Step [250/15966], Loss: 43.37933 \n",
      "\n",
      "Time elasped: 243 \n",
      "\n",
      "Epoch [0/60], Step [250/15966], acc: 0.17500 \n",
      "\n",
      "Epoch [0/60], Step [260/15966], Loss: 46.41529 \n",
      "\n",
      "Time elasped: 253 \n",
      "\n",
      "Epoch [0/60], Step [260/15966], acc: 0.18000 \n",
      "\n",
      "Epoch [0/60], Step [270/15966], Loss: 46.32519 \n",
      "\n",
      "Time elasped: 263 \n",
      "\n",
      "Epoch [0/60], Step [270/15966], acc: 0.17000 \n",
      "\n",
      "Epoch [0/60], Step [280/15966], Loss: 44.94885 \n",
      "\n",
      "Time elasped: 272 \n",
      "\n",
      "Epoch [0/60], Step [280/15966], acc: 0.19500 \n",
      "\n",
      "Epoch [0/60], Step [370/15966], Loss: 45.47023 \n",
      "\n",
      "Time elasped: 360 \n",
      "\n",
      "Epoch [0/60], Step [370/15966], acc: 0.14500 \n",
      "\n",
      "Epoch [0/60], Step [380/15966], Loss: 41.85257 \n",
      "\n",
      "Time elasped: 369 \n",
      "\n",
      "Epoch [0/60], Step [380/15966], acc: 0.20500 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3,4,5,6 python -m torch.distributed.launch --nproc_per_node=4 example_ES_res18_warmup.py\n",
    "#固定第一层，训练1个epoch，然后对后面的层进行训练，batch = 21*4,warm_up acc=15.39%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "6  is ready\n",
      "2  is ready\n",
      "1  is ready\n",
      "4  is ready\n",
      "5  is ready\n",
      "3  is ready\n",
      "0  is ready\n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "start recording\n",
      "===> training models...\n",
      "Epoch [0/25], Step [10/8294], Loss: 69.68958 \n",
      "\n",
      "Time elasped: 14 \n",
      "\n",
      "Epoch [0/25], Step [10/8294], acc: 0.01818 \n",
      "\n",
      "Epoch [0/25], Step [20/8294], Loss: 67.73158 \n",
      "\n",
      "Time elasped: 32 \n",
      "\n",
      "Epoch [0/25], Step [20/8294], acc: 0.00000 \n",
      "\n",
      "Epoch [0/25], Step [30/8294], Loss: 65.68494 \n",
      "\n",
      "Time elasped: 52 \n",
      "\n",
      "Epoch [0/25], Step [30/8294], acc: 0.00909 \n",
      "\n",
      "Epoch [0/25], Step [40/8294], Loss: 65.19390 \n",
      "\n",
      "Time elasped: 71 \n",
      "\n",
      "Epoch [0/25], Step [40/8294], acc: 0.00909 \n",
      "\n",
      "Epoch [0/25], Step [50/8294], Loss: 64.10764 \n",
      "\n",
      "Time elasped: 90 \n",
      "\n",
      "Epoch [0/25], Step [50/8294], acc: 0.00000 \n",
      "\n",
      "Epoch [0/25], Step [60/8294], Loss: 63.05705 \n",
      "\n",
      "Time elasped: 108 \n",
      "\n",
      "Epoch [0/25], Step [60/8294], acc: 0.01364 \n",
      "\n",
      "Epoch [0/25], Step [70/8294], Loss: 61.90129 \n",
      "\n",
      "Time elasped: 127 \n",
      "\n",
      "Epoch [0/25], Step [70/8294], acc: 0.00455 \n",
      "\n",
      "Epoch [0/25], Step [80/8294], Loss: 61.61550 \n",
      "\n",
      "Time elasped: 145 \n",
      "\n",
      "Epoch [0/25], Step [80/8294], acc: 0.01364 \n",
      "\n",
      "Epoch [0/25], Step [90/8294], Loss: 60.90254 \n",
      "\n",
      "Time elasped: 164 \n",
      "\n",
      "Epoch [0/25], Step [90/8294], acc: 0.01818 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6 python -m torch.distributed.launch --nproc_per_node=7 example_ES_res18_pretrain.py\n",
    "#pretrain batchsize = 22*7 = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "2  is ready\n",
      "4  is ready\n",
      "0  is ready\n",
      "6  is ready\n",
      "5  is ready\n",
      "1  is ready\n",
      "3  is ready\n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "Total number of paramerters in networks is 11693233  \n",
      "start recording\n",
      "===> training models...\n",
      "Epoch [0/25], Step [10/9123], Loss: 69.75104 \n",
      "\n",
      "Time elasped: 13 \n",
      "\n",
      "Epoch [0/25], Step [10/9123], acc: 0.00000 \n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/distributed/launch.py\", line 261, in <module>\n",
      "    main()\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/distributed/launch.py\", line 254, in main\n",
      "    process.wait()\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/subprocess.py\", line 1477, in wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/subprocess.py\", line 1424, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"example_ES_res18.py\", line 137, in <module>\n",
      "    for i, (images, labels) in enumerate(train_loader):\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "Traceback (most recent call last):\n",
      "  File \"example_ES_res18.py\", line 137, in <module>\n",
      "    for i, (images, labels) in enumerate(train_loader):\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "    data = self._next_data()\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "KeyboardInterrupt\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"../datasets/es_imagenet.py\", line 43, in __getitem__\n",
      "    datapos = np.load(filename)['pos'].astype(np.float64)\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 423, in load\n",
      "    magic = fid.read(N)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"example_ES_res18.py\", line 137, in <module>\n",
      "    for i, (images, labels) in enumerate(train_loader):\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"example_ES_res18.py\", line 137, in <module>\n",
      "    for i, (images, labels) in enumerate(train_loader):\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"example_ES_res18.py\", line 137, in <module>\n",
      "    for i, (images, labels) in enumerate(train_loader):\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/lyh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6 python -m torch.distributed.launch --nproc_per_node=7 example_ES_res18.py\n",
    "#pretrain batchsize = 22*7,acc=42.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
