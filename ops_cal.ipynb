{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.distributed as dist \n",
    "import torch.nn as nn\n",
    "import LIAF,LIAFnet\n",
    "import argparse, pickle, torch, time, os,sys\n",
    "from importlib import import_module\n",
    "from LIAFnet.LIAFResNet import *\n",
    "\n",
    "\n",
    "modules = import_module('LIAFnet.LIAFResNet_34')\n",
    "config  = modules.Config()\n",
    "snn34 = LIAFResNet(config)\n",
    "\n",
    "modules = import_module('LIAFnet.LIAFResNet_18')\n",
    "config  = modules.Config()\n",
    "snn18 = LIAFResNet(config)\n",
    "\n",
    "from LIAFnet.ResNet2D import *\n",
    "cnn2d18 = resnet18()\n",
    "cnn2d34 = resnet34()\n",
    "\n",
    "from LIAFnet.ResNet3D import *\n",
    "\n",
    "cnn3d18 = resnet18()\n",
    "cnn3d34 = resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LIAF import *\n",
    "from LIAFnet.LIAFResNet import *\n",
    "from torch.nn import BatchNorm3d,Conv2d,Linear,Conv3d\n",
    "\n",
    "\n",
    "time_windows = 8\n",
    "feature_size = {2:224,64:55,128:28,256:14,512:7,1:224}#chennel:size\n",
    "feature_size_2 = {64:4,128:2,256:1,512:1}\n",
    "\n",
    "def cal_LIAFConvCell(m,isliaf=True):\n",
    "        print(type(m))\n",
    "        kernel_size = m.kernel.kernel_size[0]\n",
    "        inchannel =  m.kernel.in_channels\n",
    "        outchennel = m.kernel.out_channels\n",
    "        stride = m.kernel.stride[0]\n",
    "        padding = m.kernel.padding[0]\n",
    "        n = feature_size[inchannel]\n",
    "        fp32mut_this = 0\n",
    "        fp32plus_this = 0\n",
    "        #Conv\n",
    "        outsize = math.floor((n + 2 * padding - kernel_size) / stride + 1)\n",
    "        if isliaf:\n",
    "            fp32mut_this += inchannel * outchennel * outsize**2*kernel_size**2 \n",
    "        fp32plus_this += inchannel * outchennel * outsize**2*(kernel_size**2-1)\n",
    "        #U_update \n",
    "        fp32plus_this += outchennel * outsize**2\n",
    "        #U_spike\n",
    "        fp32plus_this += outchennel * outsize**2\n",
    "        fp32mut_this += 2*outchennel * outsize**2\n",
    "        print('intput:',n,'*',n, 'output:',outsize,'*',outsize)\n",
    "        return fp32plus_this*time_windows ,fp32mut_this*time_windows\n",
    "\n",
    "def cal_Temporal_Conv2d(m,isliaf=True):\n",
    "        print(type(m))\n",
    "        kernel_size = m.kernel_size[0]\n",
    "        inchannel =  m.in_channels\n",
    "        outchennel = m.out_channels\n",
    "        stride = m.stride[0]\n",
    "        padding = m.padding[0]\n",
    "        n = feature_size[inchannel]\n",
    "        fp32mut_this = 0\n",
    "        fp32plus_this = 0\n",
    "        #Conv\n",
    "        outsize = math.floor((n + 2 * padding - kernel_size) / stride + 1)\n",
    "        if isliaf:\n",
    "            fp32mut_this += inchannel * outchennel * outsize**2*kernel_size**2 \n",
    "        fp32plus_this += inchannel * outchennel * outsize**2*(kernel_size**2-1)\n",
    "        #U_update \n",
    "        fp32plus_this += outchennel * outsize**2\n",
    "        #U_spike\n",
    "        fp32plus_this += outchennel * outsize**2\n",
    "        fp32mut_this += 2*outchennel * outsize**2\n",
    "        print('intput:',n,'*',n, 'output:',outsize,'*',outsize)\n",
    "        return fp32plus_this*time_windows, fp32mut_this*time_windows\n",
    "    \n",
    "def cal_Conv2d(m):\n",
    "        print(type(m))\n",
    "        kernel_size = m.kernel_size[0]\n",
    "        inchannel =  m.in_channels\n",
    "        outchennel = m.out_channels\n",
    "        stride = m.stride[0]\n",
    "        padding = m.padding[0]\n",
    "        n = feature_size[inchannel]\n",
    "        fp32mut_this = 0\n",
    "        fp32plus_this = 0\n",
    "        #Conv\n",
    "        outsize = math.floor((n + 2 * padding - kernel_size) / stride + 1)\n",
    "        fp32mut_this +=  outchennel *(inchannel *kernel_size**2+1) * outsize**2\n",
    "        fp32plus_this += inchannel * outchennel * outsize**2*(kernel_size**2-1)\n",
    "        print('intput:',n,'*',n, 'output:',outsize,'*',outsize,'   ',inchannel,'to',outchennel)\n",
    "        return fp32plus_this, fp32mut_this\n",
    "    \n",
    "def cal_Conv3d(m):\n",
    "        print(type(m))\n",
    "        kernel_size = m.kernel_size[1]\n",
    "        inchannel =  m.in_channels\n",
    "        outchennel = m.out_channels\n",
    "        stride = m.stride[1]\n",
    "        padding = m.padding[1]\n",
    "        \n",
    "        depth =  m.kernel_size[0]\n",
    "        timestride = m.stride[0]\n",
    "        timespadding = m.stride[0]\n",
    "        \n",
    "        n = feature_size[inchannel]\n",
    "        fp32mut_this = 0\n",
    "        fp32plus_this = 0\n",
    "        #Conv\n",
    "        outtime = feature_size_2[outchennel]\n",
    "        outsize = math.floor((n + 2 * padding - kernel_size) / stride + 1)\n",
    "        \n",
    "        fp32mut_this += inchannel * outchennel * outsize**2*outtime * kernel_size**2*depth\n",
    "        fp32plus_this += inchannel * outchennel * outsize**2*outtime * (kernel_size**2*depth-1)\n",
    "        print('intput:',time_windows,'*',n,'*',n, 'output:',outtime,'*',outsize,'*',outsize,'   ',inchannel,'to',outchennel)\n",
    "        return fp32plus_this, fp32mut_this\n",
    "\n",
    "\n",
    "def cal_Linear(m):\n",
    "        print(type(m))\n",
    "        input_size = m.in_features\n",
    "        hedden_size =  m.out_features\n",
    "        #Conv\n",
    "        fp32mut_this = 0\n",
    "        fp32plus_this = 0\n",
    "        fp32mut_this += input_size*input_size * input_size\n",
    "        fp32plus_this += input_size*input_size * input_size\n",
    "        print('intput:',input_size,'*',input_size, 'output:',hedden_size,'*',hedden_size)\n",
    "        return fp32plus_this, fp32mut_this\n",
    "\n",
    "isliaf = False\n",
    "def cal_ops(model):\n",
    "    fp32plus = 0\n",
    "    fp32mut = 0\n",
    "    for idx,m in enumerate(model.modules()):\n",
    "        if isinstance(m,LIAFResNet) or isinstance(m,nn.Sequential):\n",
    "            continue\n",
    "        if isinstance(m,LIAFConvCell) :\n",
    "            fp32plus_this,fp32mut_this = cal_LIAFConvCell(m,isliaf)\n",
    "            print('fp32+=',fp32plus_this,'fp32*=',fp32mut_this)\n",
    "        elif isinstance(m,Temporal_Conv2d):\n",
    "            fp32plus_this,fp32mut_this = cal_Temporal_Conv2d(m,isliaf)\n",
    "            print('fp32+=',fp32plus_this,'fp32*=',fp32mut_this)\n",
    "        elif isinstance(m,Conv2d):\n",
    "            fp32plus_this,fp32mut_this = cal_Conv2d(m)\n",
    "            print('fp32+=',fp32plus_this,'fp32*=',fp32mut_this)\n",
    "        elif isinstance(m,Linear):\n",
    "            fp32plus_this,fp32mut_this = cal_Linear(m)\n",
    "            print('fp32+=',fp32plus_this,'fp32*=',fp32mut_this)\n",
    "        elif isinstance(m,Conv3d):\n",
    "            fp32plus_this,fp32mut_this = cal_Conv3d(m)\n",
    "            print('fp32+=',fp32plus_this,'fp32*=',fp32mut_this)\n",
    "        else:\n",
    "            continue\n",
    "        fp32plus += fp32plus_this\n",
    "        fp32mut += fp32mut_this\n",
    "    return fp32plus , fp32mut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- LIAF18 fp32+= 12668145664 fp32*= 14159104000\n",
    "- LIAF34 fp32+= 25783130112 fp32*= 28901059584\n",
    "- 3D-18  fp32+= 12082982912 fp32*= 12492945408\n",
    "- 3D-34  fp32+= 20671672320 fp32*= 21411969024\n",
    "- LIF18  fp32+= 12668145664 fp32*= 268636160\n",
    "- LIF34  fp32+= 25783130112 fp32*= 288479232\n",
    "- 2D-18  fp32+= 1574797312 fp32*= 1770123264\n",
    "- 2D-34  fp32+= 3211689984 fp32*= 3611627520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 224 * 224 output: 112 * 112     1 to 64\n",
      "fp32+= 38535168 fp32*= 40140800\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 55 * 55 output: 28 * 28     64 to 64\n",
      "fp32+= 25690112 fp32*= 28951552\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 55 * 55 output: 55 * 55     64 to 64\n",
      "fp32+= 99123200 fp32*= 111707200\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 55 * 55 output: 55 * 55     64 to 64\n",
      "fp32+= 99123200 fp32*= 111707200\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 55 * 55 output: 55 * 55     64 to 64\n",
      "fp32+= 99123200 fp32*= 111707200\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 55 * 55 output: 28 * 28     64 to 128\n",
      "fp32+= 51380224 fp32*= 57903104\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 28 * 28 output: 28 * 28     128 to 128\n",
      "fp32+= 102760448 fp32*= 115705856\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 55 * 55 output: 28 * 28     64 to 128\n",
      "fp32+= 0 fp32*= 6522880\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 28 * 28 output: 28 * 28     128 to 128\n",
      "fp32+= 102760448 fp32*= 115705856\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 28 * 28 output: 28 * 28     128 to 128\n",
      "fp32+= 102760448 fp32*= 115705856\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 28 * 28 output: 14 * 14     128 to 256\n",
      "fp32+= 51380224 fp32*= 57852928\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 14 * 14 output: 14 * 14     256 to 256\n",
      "fp32+= 102760448 fp32*= 115655680\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 28 * 28 output: 14 * 14     128 to 256\n",
      "fp32+= 0 fp32*= 6472704\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 14 * 14 output: 14 * 14     256 to 256\n",
      "fp32+= 102760448 fp32*= 115655680\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 14 * 14 output: 14 * 14     256 to 256\n",
      "fp32+= 102760448 fp32*= 115655680\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 14 * 14 output: 7 * 7     256 to 512\n",
      "fp32+= 51380224 fp32*= 57827840\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 7 * 7 output: 7 * 7     512 to 512\n",
      "fp32+= 102760448 fp32*= 115630592\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 14 * 14 output: 7 * 7     256 to 512\n",
      "fp32+= 0 fp32*= 6447616\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 7 * 7 output: 7 * 7     512 to 512\n",
      "fp32+= 102760448 fp32*= 115630592\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "intput: 7 * 7 output: 7 * 7     512 to 512\n",
      "fp32+= 102760448 fp32*= 115630592\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "intput: 512 * 512 output: 1000 * 1000\n",
      "fp32+= 134217728 fp32*= 134217728\n",
      "<class 'LIAFnet.ResNet2D.ResNet2D'> fp32+= 1574797312 fp32*= 1772435136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#fp32plus , fp32mut = cal_ops(snn)\n",
    "#print(type(snn),'fp32+=',fp32plus,'fp32*=',fp32mut)\n",
    "isliaf = False\n",
    "fp32plus , fp32mut = cal_ops(cnn2d18)\n",
    "print(type(cnn2d18),'fp32+=',fp32plus,'fp32*=',fp32mut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 4, 55, 55])\n",
      "torch.Size([1, 64, 4, 55, 55])\n",
      "torch.Size([1, 64, 4, 55, 55])\n",
      "torch.Size([1, 64, 4, 55, 55])\n",
      "torch.Size([1, 128, 2, 28, 28])\n",
      "torch.Size([1, 128, 2, 28, 28])\n",
      "torch.Size([1, 128, 2, 28, 28])\n",
      "torch.Size([1, 128, 2, 28, 28])\n",
      "torch.Size([1, 256, 1, 14, 14])\n",
      "torch.Size([1, 256, 1, 14, 14])\n",
      "torch.Size([1, 256, 1, 14, 14])\n",
      "torch.Size([1, 256, 1, 14, 14])\n",
      "torch.Size([1, 512, 1, 7, 7])\n",
      "torch.Size([1, 512, 1, 7, 7])\n",
      "torch.Size([1, 512, 1, 7, 7])\n",
      "torch.Size([1, 512, 1, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8691e-02,  3.9335e-03,  2.6532e-02,  3.3640e-03,  3.9258e-02,\n",
       "         -4.2711e-02, -2.8663e-02, -6.4670e-03, -2.1399e-02, -3.9675e-02,\n",
       "         -3.0206e-02,  2.8576e-02, -1.1526e-02, -3.8382e-02, -3.4531e-02,\n",
       "         -1.4308e-02, -2.7699e-02,  4.2343e-03, -1.3172e-02,  1.9549e-02,\n",
       "          3.5606e-02, -8.6369e-03,  3.9756e-02,  2.3853e-02, -3.0713e-02,\n",
       "         -1.9565e-03, -3.5206e-02, -2.4373e-02, -3.3446e-02, -3.1833e-02,\n",
       "         -1.4571e-02, -2.6690e-02, -7.5055e-03, -9.1792e-03,  8.2165e-03,\n",
       "          1.7861e-02,  2.3602e-02,  1.1375e-02,  4.2980e-02,  6.1698e-05,\n",
       "          2.2729e-02,  1.5666e-02,  1.9304e-02, -1.3597e-03, -1.2662e-03,\n",
       "          3.8828e-02, -3.5047e-04, -8.4883e-03, -3.7326e-02,  5.2753e-03,\n",
       "          2.1000e-02, -3.9837e-02, -3.3983e-02,  2.8843e-02, -4.4189e-04,\n",
       "          3.9028e-02,  4.7019e-03,  3.6727e-02, -3.3100e-02, -2.4353e-03,\n",
       "         -1.5840e-02, -7.5066e-03,  3.5507e-02,  2.5336e-02,  2.6135e-02,\n",
       "          2.9095e-02, -4.3934e-02, -1.9281e-02, -2.7651e-02,  1.7079e-02,\n",
       "          3.2257e-02, -6.5267e-03,  3.0713e-03,  1.6063e-02, -7.7834e-03,\n",
       "          1.4353e-02,  4.2487e-02,  1.9867e-02, -1.1094e-02,  1.9380e-02,\n",
       "          3.2826e-02, -1.4367e-02, -3.7063e-02,  5.0738e-03,  2.5817e-02,\n",
       "         -3.8498e-04, -7.8649e-04,  2.4275e-02,  3.1844e-03, -2.4600e-02,\n",
       "          9.1829e-03, -3.7103e-02, -3.8027e-04,  9.2422e-03,  3.3452e-02,\n",
       "          3.8510e-02,  3.0704e-02, -3.4279e-02,  4.3658e-02, -1.8221e-02,\n",
       "          2.5408e-02, -8.5116e-03,  2.3261e-02, -4.4242e-03, -2.6526e-02,\n",
       "         -3.2140e-02, -9.8868e-03, -4.1056e-02, -1.5181e-02, -3.1424e-02,\n",
       "          2.7343e-03, -2.2984e-02, -1.9402e-02, -1.9559e-02,  3.2996e-02,\n",
       "         -3.9377e-02, -2.6050e-02, -2.8269e-02,  2.1700e-02,  2.4749e-03,\n",
       "         -3.8955e-02,  1.9780e-02,  2.7168e-02,  1.8009e-02,  4.2227e-02,\n",
       "         -3.2089e-02,  2.6193e-02, -3.0055e-02,  1.7329e-02, -4.1761e-02,\n",
       "          1.5963e-03,  3.6539e-03, -2.5673e-02, -5.0091e-03, -1.0448e-02,\n",
       "          4.0749e-02,  1.7002e-03, -4.1413e-02,  1.0973e-02,  4.4129e-02,\n",
       "         -2.2342e-02,  2.9821e-02, -2.5729e-02, -3.3103e-02,  7.9215e-05,\n",
       "          3.9093e-02,  1.2787e-02, -2.5083e-02, -2.0488e-02, -3.0233e-02,\n",
       "         -1.4908e-02, -6.6057e-03,  1.4968e-02,  1.9223e-02, -1.4033e-04,\n",
       "          4.0442e-02,  3.3382e-02,  6.1110e-03,  2.1639e-03, -3.4522e-02,\n",
       "         -2.5849e-02,  5.0708e-03,  3.4597e-02, -2.9641e-02,  1.1236e-02,\n",
       "          1.5554e-02,  1.9149e-03,  2.4709e-04,  4.1400e-02,  2.0204e-02,\n",
       "         -1.9813e-02, -1.8724e-02, -4.3064e-02,  2.8945e-02,  2.6180e-02,\n",
       "          2.5916e-03,  2.6890e-02,  1.4311e-02,  9.9094e-04, -1.4168e-02,\n",
       "         -1.7021e-02, -4.0111e-02, -6.5176e-03,  2.0147e-03,  4.3807e-02,\n",
       "         -5.4104e-03, -3.5604e-03,  2.8529e-02,  3.0675e-02,  1.3359e-02,\n",
       "         -1.6474e-02,  2.6637e-02,  1.2394e-02, -2.8456e-02, -1.2078e-02,\n",
       "         -2.6341e-02, -6.4621e-03, -2.6379e-03,  6.1367e-03,  2.3395e-02,\n",
       "         -1.6613e-02,  2.6141e-02,  3.5241e-02, -1.0121e-02,  3.5750e-02,\n",
       "         -3.0391e-02, -1.4381e-02,  1.2306e-02, -2.3164e-02,  3.5842e-02,\n",
       "         -2.8010e-02,  1.0676e-02,  3.8454e-03,  9.7673e-03,  4.1980e-02,\n",
       "          4.6077e-03,  2.3928e-02, -4.9935e-03, -2.7273e-02, -7.7894e-03,\n",
       "         -1.1471e-02,  2.8059e-02,  3.3388e-02, -3.3666e-03, -1.8603e-02,\n",
       "          4.2987e-02,  3.7965e-02,  3.6215e-02,  3.1032e-02,  3.7332e-02,\n",
       "         -1.4949e-02,  4.1682e-03,  2.0116e-02, -3.6690e-02, -1.5018e-02,\n",
       "         -3.6395e-02, -4.1787e-02,  4.2249e-02, -1.9384e-02,  2.2675e-02,\n",
       "         -1.8207e-02, -3.0673e-02, -2.1789e-03,  2.5083e-02, -3.8852e-02,\n",
       "          1.3781e-02,  1.5022e-02,  1.0987e-02,  3.3805e-02,  4.1078e-02,\n",
       "         -1.4045e-02, -1.8954e-02, -9.0769e-03,  2.4761e-02,  3.1995e-02,\n",
       "          2.8540e-02,  4.0498e-02,  1.5205e-02,  4.9438e-03, -1.7083e-02,\n",
       "         -1.0374e-02, -2.4366e-03, -1.5011e-02,  4.3400e-02,  1.4605e-02,\n",
       "          2.1968e-02, -9.0970e-03, -4.1013e-02, -4.0623e-02, -1.9507e-02,\n",
       "         -2.2800e-02, -3.0736e-02,  1.2749e-02, -3.3534e-03, -2.4898e-02,\n",
       "         -2.4409e-03, -3.7982e-02, -1.8164e-02, -2.2578e-02, -1.7785e-02,\n",
       "         -3.2547e-02,  1.7637e-02, -3.9087e-02, -2.9492e-02,  4.2242e-02,\n",
       "          2.8229e-02, -2.2563e-02,  3.0250e-02, -5.1555e-03,  1.6699e-02,\n",
       "          2.8012e-02,  2.0772e-02,  2.3585e-03,  2.4726e-03, -1.1162e-02,\n",
       "          9.6091e-03, -1.4359e-02,  3.8933e-02,  3.7089e-02, -1.2032e-02,\n",
       "         -1.3684e-02, -9.9752e-03, -2.6690e-02,  1.4321e-02,  6.6455e-03,\n",
       "         -3.4435e-02, -1.0975e-02, -1.3028e-03, -3.9688e-02, -3.2972e-02,\n",
       "          2.1437e-02,  6.5822e-03,  9.5586e-03, -1.1214e-02, -4.8497e-03,\n",
       "          3.0480e-02,  2.9869e-02,  2.4751e-02,  1.5386e-02,  3.9319e-02,\n",
       "          1.4912e-02, -3.1094e-02, -2.3789e-02,  3.2633e-02,  1.9765e-02,\n",
       "          3.5528e-02, -1.3303e-02, -4.3203e-02,  4.0156e-02, -3.7726e-02,\n",
       "          2.4238e-02,  1.1927e-02,  1.6963e-02, -3.1330e-02, -2.5194e-02,\n",
       "          1.4446e-02, -2.4377e-02, -3.4592e-02,  2.4970e-02, -1.6281e-02,\n",
       "          4.3195e-02, -4.3127e-03,  2.6222e-02, -3.5846e-02,  1.0796e-02,\n",
       "          2.0786e-02, -7.0888e-03,  2.3504e-02,  3.0813e-02, -2.7706e-02,\n",
       "          2.0169e-02, -1.3886e-02,  5.6318e-03,  2.7278e-02, -4.7660e-03,\n",
       "         -1.1560e-02, -1.1117e-02,  2.4961e-02,  1.5207e-02,  3.7911e-02,\n",
       "          4.0588e-02, -1.6092e-02, -5.7346e-03,  4.3678e-03, -9.0613e-03,\n",
       "          4.1722e-02,  2.5185e-02,  3.5961e-02, -3.0657e-02,  2.4597e-02,\n",
       "          4.2447e-04,  4.2976e-02,  4.0803e-03,  2.7773e-02,  2.0265e-02,\n",
       "         -3.4463e-02, -2.6538e-02,  1.2371e-02,  1.9870e-02, -1.5277e-02,\n",
       "          6.9454e-03,  6.2161e-03, -2.7253e-02,  8.3859e-03, -1.9752e-02,\n",
       "         -1.6258e-02,  8.0081e-03, -4.0183e-02, -3.3900e-02, -3.8481e-02,\n",
       "         -3.6238e-02,  2.1157e-03, -2.0256e-02,  3.2280e-03,  9.7407e-03,\n",
       "          2.5220e-02,  2.5645e-02,  1.8178e-02, -2.8701e-02,  1.3380e-02,\n",
       "          3.3819e-02,  2.3787e-02, -4.3870e-02, -2.2521e-02, -2.7886e-02,\n",
       "          1.1501e-02,  3.6944e-02,  1.7872e-02,  9.6701e-03, -2.3027e-02,\n",
       "         -2.2382e-02,  3.1793e-02, -1.9775e-02,  1.1421e-02,  3.6639e-02,\n",
       "          3.2536e-02, -3.0472e-02,  3.3390e-02, -3.1108e-02,  1.1701e-02,\n",
       "         -1.9233e-02, -2.5107e-03, -4.2453e-02, -8.0660e-03, -3.5099e-02,\n",
       "          2.6916e-02,  1.1568e-02, -1.9332e-02, -3.9780e-02,  2.5505e-02,\n",
       "         -3.4032e-02,  4.6052e-03, -2.1677e-02,  2.2614e-02,  1.2024e-02,\n",
       "         -1.9718e-02, -4.3653e-02,  1.7352e-02, -4.0202e-02, -6.7039e-03,\n",
       "         -3.5771e-02, -6.8784e-03, -1.4185e-02,  1.9278e-02,  2.3261e-02,\n",
       "         -1.3399e-02,  1.6871e-02,  3.6325e-02, -4.0051e-02,  2.2042e-02,\n",
       "         -3.5360e-02, -2.3041e-02, -1.1687e-02,  2.3023e-03, -3.1410e-02,\n",
       "         -2.2006e-02, -1.0976e-02,  1.7927e-02, -2.0973e-03,  1.2090e-03,\n",
       "          5.8594e-03, -1.7824e-02, -7.2857e-03,  4.0456e-02,  2.5354e-03,\n",
       "         -3.6699e-02, -9.4938e-03, -3.0836e-02,  4.3895e-03,  1.8391e-02,\n",
       "         -1.7069e-02, -3.0591e-02,  3.6683e-02,  3.3727e-02,  2.7781e-02,\n",
       "          4.3540e-02, -4.1272e-02, -1.2622e-02, -9.6629e-03,  2.0028e-02,\n",
       "         -6.0047e-03, -1.7748e-02, -2.2556e-02,  3.6459e-02, -7.6305e-03,\n",
       "          2.3064e-02,  2.2089e-02,  7.1581e-03, -4.2850e-02, -3.3672e-02,\n",
       "         -3.2504e-02,  1.4332e-02,  2.6997e-02,  9.1424e-04, -2.7099e-03,\n",
       "          3.2739e-02, -1.3289e-02, -3.5288e-02, -3.4850e-02, -2.8623e-02,\n",
       "          3.1059e-02, -5.7989e-03,  4.6530e-03,  3.7663e-02,  2.5863e-02,\n",
       "          3.0512e-02, -1.3074e-02,  3.6307e-02,  3.0139e-02, -6.5838e-03,\n",
       "         -2.6790e-02, -3.1378e-03,  3.7880e-02,  9.3357e-03,  4.0846e-02,\n",
       "         -3.4876e-03, -2.9664e-02,  2.7138e-02,  1.0494e-02,  4.2224e-02,\n",
       "         -2.9911e-02, -2.0797e-02, -3.9943e-02,  3.5927e-02, -1.5785e-02,\n",
       "         -2.4991e-02,  1.2906e-03, -1.5208e-02, -4.1304e-02, -2.6329e-02,\n",
       "         -2.5544e-02,  1.1243e-02, -2.1257e-02, -3.9051e-02,  1.8028e-02,\n",
       "         -3.3958e-02,  1.0507e-02, -1.7979e-02, -2.0958e-02,  1.3125e-02,\n",
       "         -1.8204e-02,  4.3900e-02, -1.2772e-02,  3.2386e-03, -4.3449e-03,\n",
       "         -3.2742e-02,  3.6034e-02,  4.8269e-03, -2.4193e-02, -8.2983e-03,\n",
       "         -1.6773e-02,  2.0809e-02,  1.4972e-02,  1.8382e-02,  2.9429e-02,\n",
       "          3.7321e-05, -1.3325e-02, -2.5753e-02,  3.8891e-02, -3.9525e-03,\n",
       "          8.2159e-04,  1.6668e-02, -9.6262e-04,  1.8078e-02,  1.5274e-02,\n",
       "          2.7896e-02,  3.1259e-02,  3.8142e-02, -4.0037e-02, -1.5545e-02,\n",
       "          6.4424e-03, -1.7553e-02, -3.4639e-02,  2.0725e-02, -9.8326e-03,\n",
       "          9.4647e-03,  3.6711e-04,  1.7735e-02,  1.2959e-02,  3.5339e-02,\n",
       "          3.4610e-02,  3.9621e-02,  4.2136e-02, -4.0507e-02, -3.6301e-02,\n",
       "         -3.0077e-02,  3.3659e-02,  3.9336e-02, -2.8654e-02, -2.7258e-02,\n",
       "         -3.0364e-02,  1.8159e-02,  4.1656e-02, -1.2900e-02, -2.0686e-02,\n",
       "         -4.4043e-02, -1.4466e-02,  2.0901e-02,  2.0835e-02,  2.3245e-02,\n",
       "          3.5835e-02,  1.9710e-02,  1.6941e-02,  1.4948e-02, -2.9910e-02,\n",
       "         -1.1300e-02, -4.1344e-04,  1.5091e-02, -1.7171e-02,  1.2428e-02,\n",
       "         -4.1311e-02,  1.2535e-02,  4.1229e-02, -2.4604e-02,  4.3206e-02,\n",
       "          1.4041e-02,  4.2827e-02, -2.5561e-02,  2.5852e-02, -2.5986e-02,\n",
       "          2.7356e-02, -2.3873e-02,  5.6696e-03,  4.0977e-02, -1.0551e-02,\n",
       "          4.2635e-02, -2.4811e-02, -1.6924e-02, -1.8150e-02,  8.8052e-03,\n",
       "          1.3740e-02,  1.1067e-02, -1.9905e-03,  3.8534e-02,  3.6493e-02,\n",
       "          4.4800e-03,  2.6159e-02, -3.5274e-02, -2.9546e-02, -3.8541e-03,\n",
       "          5.4598e-03,  1.0844e-02,  4.1416e-02,  2.0469e-02, -3.9801e-02,\n",
       "         -3.5634e-02, -3.9179e-02,  3.0891e-02, -3.2919e-02, -3.7548e-02,\n",
       "         -3.3847e-02, -2.4155e-02,  1.5265e-02, -3.3596e-02,  3.7959e-02,\n",
       "         -2.5780e-03,  4.3555e-02,  3.3281e-02, -2.6011e-02, -3.4501e-02,\n",
       "         -1.4047e-02,  2.6187e-02,  4.3633e-02, -3.4320e-02,  1.6401e-02,\n",
       "          3.7361e-02,  1.8811e-02,  2.2014e-02,  2.0877e-02, -3.1088e-02,\n",
       "         -3.8417e-02,  1.9570e-03,  1.3758e-02,  3.8112e-02, -3.4578e-02,\n",
       "         -4.3330e-02, -1.9591e-02, -2.9647e-02,  2.7916e-02,  4.1121e-02,\n",
       "          4.0435e-02, -9.9650e-03, -2.7829e-02,  1.3080e-02, -1.2638e-02,\n",
       "          2.6729e-03, -5.0403e-03,  3.8313e-02, -2.4360e-02,  1.3992e-02,\n",
       "         -1.5923e-02,  1.6995e-02,  1.8406e-02, -1.9147e-02,  3.3217e-02,\n",
       "         -1.9254e-02, -3.4878e-02,  3.1150e-02,  3.4866e-03, -2.5411e-02,\n",
       "          3.3122e-02, -3.8654e-02,  4.4026e-02,  3.4799e-02,  1.2132e-02,\n",
       "          1.3245e-02,  1.5829e-02, -3.4860e-02,  3.8738e-02, -3.3510e-02,\n",
       "         -2.7329e-02, -7.8015e-03,  3.1106e-02, -2.4860e-02,  3.9903e-02,\n",
       "          2.3870e-02,  1.0054e-02, -1.3211e-02,  2.6512e-02, -2.8238e-02,\n",
       "         -1.4238e-02, -8.8428e-04,  1.9459e-02, -1.2688e-02, -3.5024e-02,\n",
       "         -4.1978e-02,  5.3266e-03, -1.4376e-02,  1.0055e-02,  4.3917e-02,\n",
       "          2.2422e-02,  2.3597e-02,  2.9777e-02,  2.5940e-02,  2.8431e-02,\n",
       "         -2.9134e-02, -1.3421e-03, -2.1420e-02,  1.3869e-02,  3.3764e-02,\n",
       "          1.4719e-02,  6.8061e-04,  2.1547e-02,  3.5092e-02, -3.7412e-02,\n",
       "          3.1034e-02,  3.4654e-04,  2.0366e-02, -3.8161e-03,  1.3211e-02,\n",
       "          1.6882e-02,  5.6567e-03,  4.1061e-02,  4.0466e-02, -3.6825e-02,\n",
       "         -7.4513e-03,  2.6449e-02, -1.4862e-03, -2.2416e-02, -3.2374e-02,\n",
       "          2.3049e-02, -3.3741e-02, -1.0189e-02, -3.6965e-02,  1.0087e-02,\n",
       "          2.1196e-02, -2.1212e-02,  4.9049e-03,  1.1638e-02,  1.2112e-02,\n",
       "         -2.2934e-02, -3.7973e-02, -1.0597e-02,  2.7723e-02,  2.7315e-02,\n",
       "          2.4517e-02,  4.2894e-02,  3.6481e-02,  1.5526e-02,  3.9279e-03,\n",
       "          1.4225e-02,  1.8317e-02, -3.4167e-02, -3.3440e-02,  1.3832e-02,\n",
       "         -3.4499e-02,  3.0271e-02, -4.2967e-02,  3.4942e-02, -1.4183e-02,\n",
       "         -7.1293e-03,  2.2884e-02, -3.7792e-02,  2.0880e-04, -2.2353e-02,\n",
       "         -4.3698e-02,  3.8877e-02,  2.6317e-02,  4.2233e-02, -3.8323e-02,\n",
       "          1.0851e-02, -3.5917e-02, -3.7259e-02,  1.3981e-03,  2.9208e-02,\n",
       "          1.2170e-02, -3.2221e-02, -3.9144e-02, -1.3006e-02,  1.5575e-02,\n",
       "         -2.2997e-02,  3.0748e-02,  3.7812e-02, -1.5404e-02,  3.0468e-02,\n",
       "          4.1033e-02,  4.2235e-02, -3.9790e-02,  3.0958e-02,  5.6191e-03,\n",
       "          4.0765e-02,  4.0815e-02,  3.0810e-02,  4.1967e-02,  1.3331e-02,\n",
       "         -3.9925e-02,  4.1385e-02, -2.3770e-02, -3.9351e-02,  1.3390e-02,\n",
       "          5.6633e-03,  3.1611e-02,  5.9028e-03,  4.0618e-02,  3.4015e-02,\n",
       "         -1.4460e-02, -1.8401e-02, -2.7490e-02,  2.0357e-02, -3.9951e-02,\n",
       "         -1.0875e-02, -1.2901e-02,  8.4109e-03,  3.5033e-02,  2.5658e-02,\n",
       "         -6.6541e-04, -2.6363e-02, -3.4182e-02, -3.5616e-02, -1.9438e-02,\n",
       "          3.0668e-02,  2.0240e-02,  1.6281e-02, -1.9034e-02,  3.1759e-02,\n",
       "          2.6582e-02, -8.0105e-04, -2.1696e-02, -2.0386e-02,  1.3537e-03,\n",
       "          1.3115e-02,  1.3554e-02, -2.3524e-02, -2.4494e-02, -1.3461e-02,\n",
       "         -4.4079e-02, -1.6407e-02, -1.8657e-02, -2.8135e-02, -2.0029e-02,\n",
       "         -4.2146e-02, -2.3981e-02,  2.8668e-02, -3.3065e-02, -1.6197e-02,\n",
       "          2.7776e-02,  1.8939e-02,  2.8764e-02,  2.3734e-02,  2.5649e-02,\n",
       "          1.0608e-02, -3.6291e-02, -2.4307e-02,  3.2384e-02,  3.2222e-02,\n",
       "         -3.4856e-02,  1.5396e-02,  2.6121e-02, -2.4409e-02, -3.9579e-02,\n",
       "         -1.3929e-02,  3.8774e-02, -1.3752e-03, -3.1600e-02,  1.8623e-02,\n",
       "         -3.0765e-02, -2.8383e-03,  4.0675e-02, -3.8455e-02,  3.9361e-02,\n",
       "          3.7230e-02,  2.2093e-03,  2.8569e-02,  2.8131e-02, -1.5152e-02,\n",
       "         -3.7810e-02, -3.6887e-03,  3.4614e-02,  3.8270e-02,  3.9137e-02,\n",
       "         -2.3295e-02, -3.2165e-03,  7.5092e-03,  2.9003e-02, -1.5927e-02,\n",
       "         -2.1546e-02, -3.7332e-03,  4.9541e-03, -1.7804e-03, -1.2701e-02,\n",
       "         -3.6509e-02, -7.3009e-03,  1.1332e-02, -1.8764e-02, -2.3971e-02,\n",
       "          2.0040e-02, -2.6254e-02,  2.6167e-02,  1.1886e-02, -4.2268e-02,\n",
       "          2.9602e-02, -4.3969e-02,  6.5952e-03, -2.7875e-02, -3.4597e-02,\n",
       "         -5.5542e-03, -1.9738e-02,  1.5287e-02, -3.6526e-02,  1.8222e-02,\n",
       "         -4.0447e-02, -1.3409e-02, -6.9904e-03, -2.2205e-02,  4.2244e-02,\n",
       "         -2.8372e-02, -1.0391e-02, -3.6788e-02, -4.0744e-02,  4.0300e-02,\n",
       "          1.8028e-02,  1.0022e-02,  2.8277e-04,  3.9642e-02, -3.2940e-02,\n",
       "         -1.8509e-02, -5.3149e-03,  4.1098e-02,  2.3151e-02, -3.0556e-02,\n",
       "         -1.0235e-02,  2.4713e-02,  3.7206e-02,  4.4178e-03,  3.8324e-02,\n",
       "         -2.3716e-03, -1.8578e-02, -3.7033e-03, -2.4374e-02, -2.0820e-02,\n",
       "          1.8350e-02, -3.9505e-02,  3.1241e-02,  2.8151e-02,  1.1337e-02,\n",
       "         -3.5784e-02, -1.2631e-02,  5.8054e-03,  1.7578e-02,  4.1293e-02,\n",
       "          3.0303e-02, -2.0032e-02, -7.0491e-04, -2.6850e-03,  3.6847e-02,\n",
       "         -3.1199e-02,  8.5733e-03, -2.0410e-02, -9.1344e-03, -1.0730e-02,\n",
       "         -1.4570e-02,  3.0653e-02, -3.8745e-02, -3.7275e-02, -1.0782e-02,\n",
       "          3.7958e-02,  1.4121e-02, -3.5930e-02, -4.0607e-03, -3.9020e-02]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_random = torch.zeros([1,2,8,224,224])\n",
    "cnn3d18(input_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.BasicBlock'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.ResNet'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "3.671G\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet34 as res34\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "model = res34()\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "macs, params = clever_format([macs, params], \"%.3f\")\n",
    "print(macs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29036131501197815\n",
      "0.2918887138366699\n",
      "0.2935800850391388\n",
      "0.29492294788360596\n",
      "0.29272714257240295\n",
      "0.29272374510765076\n",
      "0.2935844957828522\n",
      "0.2924903333187103\n",
      "0.2918911576271057\n",
      "0.2905900180339813\n",
      "0.2930019199848175\n",
      "0.2920590043067932\n",
      "0.2921254336833954\n",
      "0.2924834191799164\n",
      "0.29006755352020264\n",
      "0.29061999917030334\n",
      "0.2900073826313019\n",
      "0.2907888889312744\n",
      "0.2890413701534271\n",
      "0.292036771774292\n",
      "0.28947168588638306\n",
      "0.2919151186943054\n",
      "0.29037272930145264\n",
      "0.29627084732055664\n",
      "0.2918362617492676\n",
      "0.2917218506336212\n",
      "0.2918252944946289\n",
      "0.2919565439224243\n",
      "0.2904450297355652\n",
      "0.290342777967453\n",
      "0.2904481291770935\n",
      "0.2896101772785187\n",
      "0.2894973158836365\n",
      "0.2911831736564636\n",
      "0.2882803976535797\n",
      "0.29161253571510315\n",
      "0.28969037532806396\n",
      "0.2891142666339874\n",
      "0.29127237200737\n",
      "0.291393905878067\n",
      "0.29067811369895935\n",
      "0.28896647691726685\n",
      "0.2915094494819641\n",
      "0.2876144051551819\n",
      "0.2907082140445709\n",
      "0.2945060431957245\n",
      "0.2905194163322449\n",
      "0.2930155098438263\n",
      "0.2865414321422577\n",
      "0.2886667847633362\n",
      "0.29288214445114136\n",
      "0.2930958569049835\n",
      "0.2915581166744232\n",
      "0.2915426194667816\n",
      "0.2923277020454407\n",
      "0.29421529173851013\n",
      "0.2908383011817932\n",
      "0.29096946120262146\n",
      "0.28745388984680176\n",
      "0.29189634323120117\n",
      "0.2926229238510132\n",
      "0.2910009026527405\n",
      "0.2963494062423706\n",
      "0.29078036546707153\n",
      "0.29093456268310547\n",
      "0.2857148051261902\n",
      "0.2882010340690613\n",
      "0.2851875126361847\n",
      "0.2855253517627716\n",
      "0.2918739914894104\n",
      "0.28948262333869934\n",
      "0.28907740116119385\n",
      "0.28672072291374207\n",
      "0.290063738822937\n",
      "0.2896723747253418\n",
      "0.2876568138599396\n",
      "0.2828044593334198\n",
      "0.2901304066181183\n",
      "0.28599339723587036\n",
      "0.28680357336997986\n",
      "0.28473255038261414\n",
      "0.2882780432701111\n",
      "0.2817102074623108\n",
      "0.2900950014591217\n",
      "0.2843039333820343\n",
      "0.2845268249511719\n",
      "0.28725677728652954\n",
      "0.2861335277557373\n",
      "0.2871437072753906\n",
      "0.2865411937236786\n",
      "0.2907539904117584\n",
      "0.28508490324020386\n",
      "0.28827202320098877\n",
      "0.28588220477104187\n",
      "0.28435707092285156\n",
      "0.2869219481945038\n",
      "0.2825136184692383\n",
      "0.285382479429245\n",
      "0.28724411129951477\n",
      "0.2838776409626007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d8b1d5d979f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mfire_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_fire_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from util.util import lr_scheduler\n",
    "from datasets.es_imagenet_new import ESImagenet_Dataset\n",
    "import LIAF\n",
    "from LIAFnet.LIAFResNet import *\n",
    "import torch.distributed as dist \n",
    "import torch.nn as nn\n",
    "import argparse, pickle, torch, time, os,sys\n",
    "from importlib import import_module\n",
    "\n",
    "##################### Step1. Env Preparation #####################\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##################### Step2. load in dataset #####################\n",
    "modules = import_module('LIAFnet.LIAFResNet_18')\n",
    "config  = modules.Config()\n",
    "config.actFun= LIFactFun.apply\n",
    "num_epochs = config.num_epochs\n",
    "batch_size = 20\n",
    "local_rank = 0\n",
    "timeWindows = config.timeWindows\n",
    "\n",
    "epoch = 0\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "training_iter = 0\n",
    "start_epoch = 0\n",
    "acc_record = list([])\n",
    "loss_train_record = list([])\n",
    "loss_test_record = list([])\n",
    "train_dataset = ESImagenet_Dataset(mode='train',data_set_path='/data/ES-imagenet-0.18/')\n",
    "test_dataset = ESImagenet_Dataset(mode='test',data_set_path='/data/ES-imagenet-0.18/')\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1,pin_memory=True,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1,pin_memory=True)\n",
    "\n",
    "##################### Step3. establish module #####################\n",
    "snn = LIAFResNet(config)\n",
    "snn.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(snn.parameters(),\n",
    "            lr=config.learning_rate)\n",
    "\n",
    "\n",
    "################step4. training and validation ################\n",
    "\n",
    "\n",
    "def cal_fire_rate(model):\n",
    "    fire_rate = 0\n",
    "    layercount = 0\n",
    "    fire = 0\n",
    "    for idx,m in enumerate(model.modules()):\n",
    "        if isinstance(m,LIAFResNet) or isinstance(m,nn.Sequential):\n",
    "            continue\n",
    "        if isinstance(m,LIAFConvCell) or isinstance(m,Temporal_Conv2d):\n",
    "            fire = m.fire_rate\n",
    "        else:\n",
    "            continue\n",
    "        layercount+= 1\n",
    "        fire_rate += fire\n",
    "    return fire_rate/layercount\n",
    "\n",
    "fire_rate_dict = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #training\n",
    "    fire_rate_list = []\n",
    "    running_loss = 0\n",
    "    snn.train()\n",
    "    start_time = time.time() \n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if ((i+1)<=len(train_dataset)//batch_size):\n",
    "            snn.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = snn(images.type(LIAF.dtype)).cpu()\n",
    "            labels = labels.view(batch_size)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _ , predict = outputs.max(1)\n",
    "            correct += predict.eq(labels).sum()\n",
    "            total += float(predict.size(0))\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            fire_rate = cal_fire_rate(snn)\n",
    "            print(fire_rate.item())\n",
    "            fire_rate_list.append(fire_rate)\n",
    "        training_iter +=1 \n",
    "    torch.cuda.empty_cache()\n",
    "    fire_rate_dict[epoch]=fire_rate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
